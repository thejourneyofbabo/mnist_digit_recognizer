{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "195ef189",
   "metadata": {},
   "source": [
    "# MNIST Project\n",
    "> **By Jisang Yun**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "66dcb89f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision \n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8feca20d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "964bfd19",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomMNISTDataset(Dataset):\n",
    "    def __init__(self, csv_file, transform=None, is_test=False):\n",
    "        self.data_frame = pd.read_csv(csv_file)\n",
    "        self.transform = transform\n",
    "        self.is_test = is_test\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_frame)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = self.data_frame.iloc[idx]\n",
    "\n",
    "        if self.is_test:\n",
    "            Image = item.values.reshape(28, 28).astype(np.uint8)\n",
    "            label = None\n",
    "        else:\n",
    "            Image = item[1:].values.reshape(28, 28).astype(np.uint8)\n",
    "            label = item.iloc[0]\n",
    "\n",
    "        Image = transforms.ToPILImage()(Image)\n",
    "\n",
    "        if self.transform is not None:\n",
    "            Image = self.transform(Image)\n",
    "\n",
    "        if self.is_test:\n",
    "            return Image\n",
    "        else:\n",
    "            return Image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5a1bcd54",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.RandomRotation(degrees=15),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "42d781bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = CustomMNISTDataset(csv_file='./digit-recognizer/train.csv', transform=transform, is_test=False)\n",
    "test_dataset = CustomMNISTDataset(csv_file='./digit-recognizer/test.csv', transform=transform, is_test=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "6f0dbd2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Size: 42000, Test Size: 28000\n"
     ]
    }
   ],
   "source": [
    "print('Train Size: ' + str(len(train_dataset)) + ', Test Size: ' + str(len(test_dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "24e55a74",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
       "           -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
       "           -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
       "           -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n",
       "          [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
       "           -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
       "           -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
       "           -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n",
       "          [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
       "           -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
       "           -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
       "           -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n",
       "          [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
       "           -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
       "           -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
       "           -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n",
       "          [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
       "           -1.0000, -1.0000, -1.0000,  0.0745,  0.0745,  0.5059,  0.5059,\n",
       "           -0.3255, -0.4353,  0.1843, -0.7490, -1.0000, -1.0000, -1.0000,\n",
       "           -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n",
       "          [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
       "           -1.0000, -1.0000, -0.8588,  0.9922,  0.9922,  0.9922,  0.9922,\n",
       "            0.7020,  0.9294,  0.1843,  0.8118, -0.5765, -0.8824, -1.0000,\n",
       "           -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n",
       "          [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
       "           -0.8980, -0.3255,  0.9608,  0.9922,  0.9922,  0.9922,  0.9922,\n",
       "            0.9922,  0.9922,  0.9922,  0.9922,  0.9922, -0.1843, -1.0000,\n",
       "           -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n",
       "          [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -0.8745,\n",
       "            0.4039,  0.9922,  0.9922,  0.9922,  0.9922,  0.9922,  0.9922,\n",
       "            0.9922,  0.9922,  0.9922,  0.9922,  0.9922,  0.9059, -0.3333,\n",
       "           -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n",
       "          [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -0.4353,\n",
       "            0.9922,  0.9922,  0.9922,  0.9922,  0.9922, -0.1451, -0.3490,\n",
       "            0.5608,  0.9922,  0.9922,  0.9922,  0.9922,  0.9922,  0.3412,\n",
       "           -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n",
       "          [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -0.5216,  0.4980,\n",
       "            0.9922,  0.9922,  0.9922,  0.1529,  0.1529, -0.6471, -1.0000,\n",
       "           -0.9137, -0.7725,  0.5686,  0.9765,  0.9922,  0.9922,  0.6627,\n",
       "           -0.4039, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n",
       "          [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000,  0.3490,  0.9922,\n",
       "            0.9922,  0.9922,  0.5843,  0.1529, -1.0000, -1.0000, -1.0000,\n",
       "           -1.0000, -1.0000,  0.0039, -0.3490, -0.3490,  0.9922,  0.9922,\n",
       "            0.9922,  0.2000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n",
       "          [-1.0000, -1.0000, -1.0000, -1.0000, -0.9922, -0.9922,  0.3647,\n",
       "            0.9922,  0.9922, -0.3020, -0.4745, -1.0000, -1.0000, -1.0000,\n",
       "           -1.0000, -1.0000, -1.0000, -1.0000, -0.8039,  0.8824,  0.9922,\n",
       "            0.9922,  0.2000, -0.9059, -1.0000, -1.0000, -1.0000, -1.0000],\n",
       "          [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -0.6314,  0.9922,\n",
       "            0.9922,  0.9922, -0.7725, -1.0000, -1.0000, -1.0000, -1.0000,\n",
       "           -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,  0.3020,  0.9922,\n",
       "            0.9922,  0.7569, -0.8667, -1.0000, -1.0000, -1.0000, -1.0000],\n",
       "          [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -0.3725,  0.9922,\n",
       "            0.9922,  0.8824, -0.8118, -1.0000, -1.0000, -1.0000, -1.0000,\n",
       "           -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -0.4118,  0.9922,\n",
       "            0.9922,  0.9922, -0.8667, -1.0000, -1.0000, -1.0000, -1.0000],\n",
       "          [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -0.4980,  0.9922,\n",
       "            0.9922,  0.4588, -0.9451, -1.0000, -1.0000, -1.0000, -1.0000,\n",
       "           -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -0.6235,  0.9922,\n",
       "            0.9922,  0.9922, -0.8667, -1.0000, -1.0000, -1.0000, -1.0000],\n",
       "          [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000,  0.8196,  0.9922,\n",
       "            0.9922,  0.9922, -0.7725, -1.0000, -1.0000, -1.0000, -1.0000,\n",
       "           -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -0.6235,  0.9922,\n",
       "            0.9922,  0.9922, -0.8667, -1.0000, -1.0000, -1.0000, -1.0000],\n",
       "          [-1.0000, -1.0000, -1.0000, -1.0000, -0.8902,  0.9922,  0.9922,\n",
       "            0.9922,  0.9922, -0.7725, -1.0000, -1.0000, -1.0000, -1.0000,\n",
       "           -1.0000, -1.0000, -1.0000, -1.0000, -0.8745,  0.6392,  0.9922,\n",
       "            0.9922,  0.1765,  0.1765, -1.0000, -1.0000, -1.0000, -1.0000],\n",
       "          [-1.0000, -1.0000, -1.0000, -1.0000, -0.8588,  0.2784,  0.9922,\n",
       "            0.9922,  0.9922, -0.7725, -0.7725, -0.9059, -1.0000, -1.0000,\n",
       "           -1.0000, -1.0000, -1.0000, -1.0000, -0.8353,  0.2627,  0.9922,\n",
       "            0.9922,  0.9216, -0.7569, -1.0000, -1.0000, -1.0000, -1.0000],\n",
       "          [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -0.2627,\n",
       "            0.9922,  0.9922,  0.9922,  0.5686,  0.5843, -0.4824, -1.0000,\n",
       "           -1.0000, -1.0000, -1.0000, -1.0000,  0.6392,  0.9922,  0.9922,\n",
       "            0.9922,  0.3412, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n",
       "          [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -0.8824,\n",
       "            0.6157,  0.9922,  0.9922,  0.9922,  0.9922,  0.5216, -0.6235,\n",
       "           -0.6235, -0.7333, -0.6784, -0.6235,  0.9922,  0.9922,  0.9922,\n",
       "            0.9922, -0.3255, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n",
       "          [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
       "           -0.5294,  0.6627,  0.9922,  0.9922,  0.9922,  0.9922,  0.9922,\n",
       "            0.9922,  0.8275,  0.9059,  0.9922,  0.9922,  0.9922,  0.8745,\n",
       "           -0.3255, -0.9137, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n",
       "          [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
       "           -1.0000, -0.3255,  0.9059,  0.9922,  0.9922,  0.9922,  0.9922,\n",
       "            0.9922,  0.9922,  0.9922,  0.9922,  0.9922,  0.9059, -0.4510,\n",
       "           -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n",
       "          [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
       "           -1.0000, -1.0000, -0.1059,  0.9922,  0.9922,  0.9922,  0.9922,\n",
       "            0.9922,  0.9922,  0.9922,  0.9922, -0.8510, -0.8824, -1.0000,\n",
       "           -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n",
       "          [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
       "           -1.0000, -1.0000, -0.8980,  0.4275,  0.9922,  0.1451,  0.9922,\n",
       "            1.0000,  1.0000,  0.9922,  1.0000, -1.0000, -1.0000, -1.0000,\n",
       "           -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n",
       "          [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
       "           -1.0000, -1.0000, -1.0000, -1.0000, -0.9373, -1.0000, -1.0000,\n",
       "           -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
       "           -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n",
       "          [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
       "           -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
       "           -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
       "           -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n",
       "          [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
       "           -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
       "           -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
       "           -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n",
       "          [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
       "           -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
       "           -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
       "           -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000]]]),\n",
       " np.int64(0))"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "4990d888",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True, num_workers=0)\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "9725d356",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Size: torch.Size([64, 1, 28, 28])\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for example_data, example_labels in train_loader:\n",
    "    example_image = example_data[0]\n",
    "    print(\"Input Size:\", example_data.size())\n",
    "    \n",
    "    example_image_numpy = example_image.permute(1, 2, 0).numpy()  # Convert to HWC format for visualization\n",
    "\n",
    "    plt.imshow(example_image_numpy)\n",
    "    plt.title(f\"Label: {example_labels[0]}\")\n",
    "    plt.show()\n",
    "\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "d639000b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleCNN(nn.Module):\n",
    "   def __init__(self):\n",
    "       super(SimpleCNN, self).__init__()\n",
    "       # Convolutional layers\n",
    "       self.conv1 = nn.Conv2d(in_channels=1, out_channels=32, kernel_size=3, stride=1, padding=1)\n",
    "       self.conv2 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, stride=1, padding=1)\n",
    "       self.conv3 = nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, stride=1, padding=1)\n",
    "       \n",
    "       # Activation and pooling layers\n",
    "       self.relu = nn.ReLU()\n",
    "       self.pool = nn.MaxPool2d(kernel_size=2, stride=2)  # Only define once\n",
    "       \n",
    "       # Automatically calculate the input size for fully connected layer\n",
    "       self.fc_input_size = self._get_conv_output_size()\n",
    "       \n",
    "       # Fully connected layers\n",
    "       self.fc1 = nn.Linear(self.fc_input_size, 128)\n",
    "       self.dropout = nn.Dropout(0.5)\n",
    "       self.fc2 = nn.Linear(128, 20)\n",
    "       self.fc3 = nn.Linear(20, 10)\n",
    "\n",
    "   def _get_conv_output_size(self):\n",
    "       \"\"\"Automatically calculate the output size of convolutional layers\"\"\"\n",
    "       with torch.no_grad():\n",
    "           # Test with MNIST image size (1, 28, 28)\n",
    "           x = torch.zeros(1, 1, 28, 28)\n",
    "           \n",
    "           # Forward pass through conv layers\n",
    "           x = self.pool(self.relu(self.conv1(x)))  # 28x28 -> 14x14\n",
    "           x = self.pool(self.relu(self.conv2(x)))  # 14x14 -> 7x7\n",
    "           x = self.pool(self.relu(self.conv3(x)))  # 7x7 -> 3x3\n",
    "           \n",
    "           return x.view(1, -1).size(1)\n",
    "\n",
    "   def forward(self, x):\n",
    "       # Convolutional layers with ReLU and pooling\n",
    "       x = self.pool(self.relu(self.conv1(x)))  # (batch, 32, 14, 14)\n",
    "       x = self.pool(self.relu(self.conv2(x)))  # (batch, 64, 7, 7)\n",
    "       x = self.pool(self.relu(self.conv3(x)))  # (batch, 128, 3, 3)\n",
    "       \n",
    "       # Flatten the tensor for fully connected layers\n",
    "       x = x.view(x.size(0), -1)  # Safer and simpler method\n",
    "       \n",
    "       # Fully connected layers\n",
    "       x = self.relu(self.fc1(x))\n",
    "       x = self.dropout(x)\n",
    "       x = self.relu(self.fc2(x))  # Add ReLU to fc2 as well\n",
    "       x = self.fc3(x)  # Final output layer (no activation)\n",
    "       \n",
    "       return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "b460b746",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SimpleCNN().to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "# optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "47d8ebc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 100] loss: 1.559\n",
      "[1, 200] loss: 0.521\n",
      "[1, 300] loss: 0.357\n",
      "[1, 400] loss: 0.262\n",
      "[1, 500] loss: 0.221\n",
      "[1, 600] loss: 0.216\n",
      "[2, 100] loss: 0.274\n",
      "[2, 200] loss: 0.165\n",
      "[2, 300] loss: 0.150\n",
      "[2, 400] loss: 0.154\n",
      "[2, 500] loss: 0.131\n",
      "[2, 600] loss: 0.130\n",
      "[3, 100] loss: 0.205\n",
      "[3, 200] loss: 0.115\n",
      "[3, 300] loss: 0.101\n",
      "[3, 400] loss: 0.110\n",
      "[3, 500] loss: 0.094\n",
      "[3, 600] loss: 0.111\n",
      "[4, 100] loss: 0.160\n",
      "[4, 200] loss: 0.084\n",
      "[4, 300] loss: 0.096\n",
      "[4, 400] loss: 0.087\n",
      "[4, 500] loss: 0.085\n",
      "[4, 600] loss: 0.091\n",
      "[5, 100] loss: 0.125\n",
      "[5, 200] loss: 0.083\n",
      "[5, 300] loss: 0.074\n",
      "[5, 400] loss: 0.071\n",
      "[5, 500] loss: 0.078\n",
      "[5, 600] loss: 0.086\n",
      "[6, 100] loss: 0.107\n",
      "[6, 200] loss: 0.068\n",
      "[6, 300] loss: 0.066\n",
      "[6, 400] loss: 0.071\n",
      "[6, 500] loss: 0.072\n",
      "[6, 600] loss: 0.065\n",
      "[7, 100] loss: 0.095\n",
      "[7, 200] loss: 0.068\n",
      "[7, 300] loss: 0.063\n",
      "[7, 400] loss: 0.068\n",
      "[7, 500] loss: 0.066\n",
      "[7, 600] loss: 0.056\n",
      "[8, 100] loss: 0.081\n",
      "[8, 200] loss: 0.054\n",
      "[8, 300] loss: 0.047\n",
      "[8, 400] loss: 0.055\n",
      "[8, 500] loss: 0.056\n",
      "[8, 600] loss: 0.066\n",
      "[9, 100] loss: 0.087\n",
      "[9, 200] loss: 0.052\n",
      "[9, 300] loss: 0.057\n",
      "[9, 400] loss: 0.053\n",
      "[9, 500] loss: 0.051\n",
      "[9, 600] loss: 0.057\n",
      "[10, 100] loss: 0.075\n",
      "[10, 200] loss: 0.054\n",
      "[10, 300] loss: 0.050\n",
      "[10, 400] loss: 0.059\n",
      "[10, 500] loss: 0.045\n",
      "[10, 600] loss: 0.041\n",
      "[11, 100] loss: 0.067\n",
      "[11, 200] loss: 0.048\n",
      "[11, 300] loss: 0.042\n",
      "[11, 400] loss: 0.041\n",
      "[11, 500] loss: 0.047\n",
      "[11, 600] loss: 0.043\n",
      "[12, 100] loss: 0.055\n",
      "[12, 200] loss: 0.042\n",
      "[12, 300] loss: 0.041\n",
      "[12, 400] loss: 0.045\n",
      "[12, 500] loss: 0.037\n",
      "[12, 600] loss: 0.050\n",
      "[13, 100] loss: 0.067\n",
      "[13, 200] loss: 0.044\n",
      "[13, 300] loss: 0.037\n",
      "[13, 400] loss: 0.045\n",
      "[13, 500] loss: 0.040\n",
      "[13, 600] loss: 0.036\n",
      "[14, 100] loss: 0.058\n",
      "[14, 200] loss: 0.037\n",
      "[14, 300] loss: 0.025\n",
      "[14, 400] loss: 0.039\n",
      "[14, 500] loss: 0.038\n",
      "[14, 600] loss: 0.049\n",
      "[15, 100] loss: 0.060\n",
      "[15, 200] loss: 0.032\n",
      "[15, 300] loss: 0.036\n",
      "[15, 400] loss: 0.043\n",
      "[15, 500] loss: 0.040\n",
      "[15, 600] loss: 0.033\n",
      "[16, 100] loss: 0.057\n",
      "[16, 200] loss: 0.025\n",
      "[16, 300] loss: 0.039\n",
      "[16, 400] loss: 0.025\n",
      "[16, 500] loss: 0.040\n",
      "[16, 600] loss: 0.034\n",
      "[17, 100] loss: 0.046\n",
      "[17, 200] loss: 0.037\n",
      "[17, 300] loss: 0.027\n",
      "[17, 400] loss: 0.031\n",
      "[17, 500] loss: 0.037\n",
      "[17, 600] loss: 0.036\n",
      "[18, 100] loss: 0.042\n",
      "[18, 200] loss: 0.037\n",
      "[18, 300] loss: 0.030\n",
      "[18, 400] loss: 0.031\n",
      "[18, 500] loss: 0.032\n",
      "[18, 600] loss: 0.033\n",
      "[19, 100] loss: 0.049\n",
      "[19, 200] loss: 0.024\n",
      "[19, 300] loss: 0.032\n",
      "[19, 400] loss: 0.035\n",
      "[19, 500] loss: 0.032\n",
      "[19, 600] loss: 0.029\n",
      "[20, 100] loss: 0.055\n",
      "[20, 200] loss: 0.024\n",
      "[20, 300] loss: 0.029\n",
      "[20, 400] loss: 0.025\n",
      "[20, 500] loss: 0.027\n",
      "[20, 600] loss: 0.029\n",
      "[21, 100] loss: 0.042\n",
      "[21, 200] loss: 0.028\n",
      "[21, 300] loss: 0.030\n",
      "[21, 400] loss: 0.030\n",
      "[21, 500] loss: 0.030\n",
      "[21, 600] loss: 0.029\n",
      "[22, 100] loss: 0.045\n",
      "[22, 200] loss: 0.029\n",
      "[22, 300] loss: 0.026\n",
      "[22, 400] loss: 0.020\n",
      "[22, 500] loss: 0.035\n",
      "[22, 600] loss: 0.029\n",
      "[23, 100] loss: 0.043\n",
      "[23, 200] loss: 0.025\n",
      "[23, 300] loss: 0.025\n",
      "[23, 400] loss: 0.026\n",
      "[23, 500] loss: 0.037\n",
      "[23, 600] loss: 0.028\n",
      "[24, 100] loss: 0.041\n",
      "[24, 200] loss: 0.019\n",
      "[24, 300] loss: 0.032\n",
      "[24, 400] loss: 0.027\n",
      "[24, 500] loss: 0.024\n",
      "[24, 600] loss: 0.022\n",
      "[25, 100] loss: 0.039\n",
      "[25, 200] loss: 0.024\n",
      "[25, 300] loss: 0.030\n",
      "[25, 400] loss: 0.020\n",
      "[25, 500] loss: 0.025\n",
      "[25, 600] loss: 0.026\n",
      "[26, 100] loss: 0.037\n",
      "[26, 200] loss: 0.018\n",
      "[26, 300] loss: 0.023\n",
      "[26, 400] loss: 0.026\n",
      "[26, 500] loss: 0.023\n",
      "[26, 600] loss: 0.025\n",
      "[27, 100] loss: 0.037\n",
      "[27, 200] loss: 0.025\n",
      "[27, 300] loss: 0.024\n",
      "[27, 400] loss: 0.021\n",
      "[27, 500] loss: 0.023\n",
      "[27, 600] loss: 0.020\n",
      "[28, 100] loss: 0.029\n",
      "[28, 200] loss: 0.025\n",
      "[28, 300] loss: 0.022\n",
      "[28, 400] loss: 0.019\n",
      "[28, 500] loss: 0.023\n",
      "[28, 600] loss: 0.023\n",
      "[29, 100] loss: 0.029\n",
      "[29, 200] loss: 0.034\n",
      "[29, 300] loss: 0.018\n",
      "[29, 400] loss: 0.024\n",
      "[29, 500] loss: 0.028\n",
      "[29, 600] loss: 0.030\n",
      "[30, 100] loss: 0.038\n",
      "[30, 200] loss: 0.021\n",
      "[30, 300] loss: 0.017\n",
      "[30, 400] loss: 0.022\n",
      "[30, 500] loss: 0.012\n",
      "[30, 600] loss: 0.023\n",
      "[31, 100] loss: 0.033\n",
      "[31, 200] loss: 0.024\n",
      "[31, 300] loss: 0.025\n",
      "[31, 400] loss: 0.026\n",
      "[31, 500] loss: 0.020\n",
      "[31, 600] loss: 0.033\n",
      "[32, 100] loss: 0.034\n",
      "[32, 200] loss: 0.015\n",
      "[32, 300] loss: 0.016\n",
      "[32, 400] loss: 0.023\n",
      "[32, 500] loss: 0.015\n",
      "[32, 600] loss: 0.019\n",
      "[33, 100] loss: 0.034\n",
      "[33, 200] loss: 0.016\n",
      "[33, 300] loss: 0.017\n",
      "[33, 400] loss: 0.024\n",
      "[33, 500] loss: 0.022\n",
      "[33, 600] loss: 0.016\n",
      "[34, 100] loss: 0.032\n",
      "[34, 200] loss: 0.020\n",
      "[34, 300] loss: 0.024\n",
      "[34, 400] loss: 0.016\n",
      "[34, 500] loss: 0.019\n",
      "[34, 600] loss: 0.020\n",
      "[35, 100] loss: 0.027\n",
      "[35, 200] loss: 0.021\n",
      "[35, 300] loss: 0.018\n",
      "[35, 400] loss: 0.017\n",
      "[35, 500] loss: 0.021\n",
      "[35, 600] loss: 0.023\n",
      "[36, 100] loss: 0.025\n",
      "[36, 200] loss: 0.017\n",
      "[36, 300] loss: 0.013\n",
      "[36, 400] loss: 0.015\n",
      "[36, 500] loss: 0.020\n",
      "[36, 600] loss: 0.017\n",
      "[37, 100] loss: 0.032\n",
      "[37, 200] loss: 0.015\n",
      "[37, 300] loss: 0.019\n",
      "[37, 400] loss: 0.020\n",
      "[37, 500] loss: 0.021\n",
      "[37, 600] loss: 0.021\n",
      "[38, 100] loss: 0.030\n",
      "[38, 200] loss: 0.016\n",
      "[38, 300] loss: 0.015\n",
      "[38, 400] loss: 0.017\n",
      "[38, 500] loss: 0.030\n",
      "[38, 600] loss: 0.019\n",
      "[39, 100] loss: 0.023\n",
      "[39, 200] loss: 0.017\n",
      "[39, 300] loss: 0.014\n",
      "[39, 400] loss: 0.017\n",
      "[39, 500] loss: 0.028\n",
      "[39, 600] loss: 0.018\n",
      "[40, 100] loss: 0.025\n",
      "[40, 200] loss: 0.006\n",
      "[40, 300] loss: 0.011\n",
      "[40, 400] loss: 0.016\n",
      "[40, 500] loss: 0.030\n",
      "[40, 600] loss: 0.015\n",
      "[41, 100] loss: 0.026\n",
      "[41, 200] loss: 0.017\n",
      "[41, 300] loss: 0.015\n",
      "[41, 400] loss: 0.027\n",
      "[41, 500] loss: 0.023\n",
      "[41, 600] loss: 0.022\n",
      "[42, 100] loss: 0.026\n",
      "[42, 200] loss: 0.018\n",
      "[42, 300] loss: 0.015\n",
      "[42, 400] loss: 0.015\n",
      "[42, 500] loss: 0.013\n",
      "[42, 600] loss: 0.016\n",
      "[43, 100] loss: 0.031\n",
      "[43, 200] loss: 0.019\n",
      "[43, 300] loss: 0.016\n",
      "[43, 400] loss: 0.017\n",
      "[43, 500] loss: 0.015\n",
      "[43, 600] loss: 0.015\n",
      "[44, 100] loss: 0.021\n",
      "[44, 200] loss: 0.026\n",
      "[44, 300] loss: 0.017\n",
      "[44, 400] loss: 0.013\n",
      "[44, 500] loss: 0.022\n",
      "[44, 600] loss: 0.014\n",
      "[45, 100] loss: 0.025\n",
      "[45, 200] loss: 0.015\n",
      "[45, 300] loss: 0.021\n",
      "[45, 400] loss: 0.015\n",
      "[45, 500] loss: 0.016\n",
      "[45, 600] loss: 0.022\n",
      "[46, 100] loss: 0.021\n",
      "[46, 200] loss: 0.019\n",
      "[46, 300] loss: 0.017\n",
      "[46, 400] loss: 0.014\n",
      "[46, 500] loss: 0.015\n",
      "[46, 600] loss: 0.015\n",
      "[47, 100] loss: 0.024\n",
      "[47, 200] loss: 0.015\n",
      "[47, 300] loss: 0.014\n",
      "[47, 400] loss: 0.014\n",
      "[47, 500] loss: 0.013\n",
      "[47, 600] loss: 0.014\n",
      "[48, 100] loss: 0.029\n",
      "[48, 200] loss: 0.014\n",
      "[48, 300] loss: 0.013\n",
      "[48, 400] loss: 0.016\n",
      "[48, 500] loss: 0.021\n",
      "[48, 600] loss: 0.022\n",
      "[49, 100] loss: 0.019\n",
      "[49, 200] loss: 0.014\n",
      "[49, 300] loss: 0.015\n",
      "[49, 400] loss: 0.018\n",
      "[49, 500] loss: 0.018\n",
      "[49, 600] loss: 0.013\n",
      "[50, 100] loss: 0.018\n",
      "[50, 200] loss: 0.018\n",
      "[50, 300] loss: 0.015\n",
      "[50, 400] loss: 0.014\n",
      "[50, 500] loss: 0.018\n",
      "[50, 600] loss: 0.020\n",
      "[51, 100] loss: 0.029\n",
      "[51, 200] loss: 0.015\n",
      "[51, 300] loss: 0.013\n",
      "[51, 400] loss: 0.019\n",
      "[51, 500] loss: 0.014\n",
      "[51, 600] loss: 0.015\n",
      "[52, 100] loss: 0.021\n",
      "[52, 200] loss: 0.012\n",
      "[52, 300] loss: 0.014\n",
      "[52, 400] loss: 0.015\n",
      "[52, 500] loss: 0.013\n",
      "[52, 600] loss: 0.015\n",
      "[53, 100] loss: 0.017\n",
      "[53, 200] loss: 0.011\n",
      "[53, 300] loss: 0.017\n",
      "[53, 400] loss: 0.015\n",
      "[53, 500] loss: 0.013\n",
      "[53, 600] loss: 0.011\n",
      "[54, 100] loss: 0.018\n",
      "[54, 200] loss: 0.021\n",
      "[54, 300] loss: 0.019\n",
      "[54, 400] loss: 0.014\n",
      "[54, 500] loss: 0.019\n",
      "[54, 600] loss: 0.013\n",
      "[55, 100] loss: 0.018\n",
      "[55, 200] loss: 0.009\n",
      "[55, 300] loss: 0.015\n",
      "[55, 400] loss: 0.013\n",
      "[55, 500] loss: 0.021\n",
      "[55, 600] loss: 0.015\n",
      "[56, 100] loss: 0.019\n",
      "[56, 200] loss: 0.011\n",
      "[56, 300] loss: 0.013\n",
      "[56, 400] loss: 0.012\n",
      "[56, 500] loss: 0.018\n",
      "[56, 600] loss: 0.020\n",
      "[57, 100] loss: 0.027\n",
      "[57, 200] loss: 0.018\n",
      "[57, 300] loss: 0.012\n",
      "[57, 400] loss: 0.012\n",
      "[57, 500] loss: 0.011\n",
      "[57, 600] loss: 0.018\n",
      "[58, 100] loss: 0.015\n",
      "[58, 200] loss: 0.010\n",
      "[58, 300] loss: 0.014\n",
      "[58, 400] loss: 0.020\n",
      "[58, 500] loss: 0.014\n",
      "[58, 600] loss: 0.014\n",
      "[59, 100] loss: 0.020\n",
      "[59, 200] loss: 0.014\n",
      "[59, 300] loss: 0.013\n",
      "[59, 400] loss: 0.009\n",
      "[59, 500] loss: 0.013\n",
      "[59, 600] loss: 0.012\n",
      "[60, 100] loss: 0.021\n",
      "[60, 200] loss: 0.014\n",
      "[60, 300] loss: 0.014\n",
      "[60, 400] loss: 0.011\n",
      "[60, 500] loss: 0.014\n",
      "[60, 600] loss: 0.008\n",
      "[61, 100] loss: 0.018\n",
      "[61, 200] loss: 0.018\n",
      "[61, 300] loss: 0.011\n",
      "[61, 400] loss: 0.008\n",
      "[61, 500] loss: 0.009\n",
      "[61, 600] loss: 0.014\n",
      "[62, 100] loss: 0.022\n",
      "[62, 200] loss: 0.009\n",
      "[62, 300] loss: 0.010\n",
      "[62, 400] loss: 0.011\n",
      "[62, 500] loss: 0.010\n",
      "[62, 600] loss: 0.013\n",
      "[63, 100] loss: 0.019\n",
      "[63, 200] loss: 0.013\n",
      "[63, 300] loss: 0.017\n",
      "[63, 400] loss: 0.020\n",
      "[63, 500] loss: 0.014\n",
      "[63, 600] loss: 0.010\n",
      "[64, 100] loss: 0.022\n",
      "[64, 200] loss: 0.017\n",
      "[64, 300] loss: 0.012\n",
      "[64, 400] loss: 0.010\n",
      "[64, 500] loss: 0.013\n",
      "[64, 600] loss: 0.011\n",
      "[65, 100] loss: 0.018\n",
      "[65, 200] loss: 0.008\n",
      "[65, 300] loss: 0.013\n",
      "[65, 400] loss: 0.013\n",
      "[65, 500] loss: 0.012\n",
      "[65, 600] loss: 0.007\n",
      "[66, 100] loss: 0.025\n",
      "[66, 200] loss: 0.011\n",
      "[66, 300] loss: 0.019\n",
      "[66, 400] loss: 0.015\n",
      "[66, 500] loss: 0.013\n",
      "[66, 600] loss: 0.009\n",
      "[67, 100] loss: 0.024\n",
      "[67, 200] loss: 0.021\n",
      "[67, 300] loss: 0.014\n",
      "[67, 400] loss: 0.009\n",
      "[67, 500] loss: 0.010\n",
      "[67, 600] loss: 0.013\n",
      "[68, 100] loss: 0.014\n",
      "[68, 200] loss: 0.010\n",
      "[68, 300] loss: 0.008\n",
      "[68, 400] loss: 0.012\n",
      "[68, 500] loss: 0.013\n",
      "[68, 600] loss: 0.015\n",
      "[69, 100] loss: 0.015\n",
      "[69, 200] loss: 0.008\n",
      "[69, 300] loss: 0.012\n",
      "[69, 400] loss: 0.014\n",
      "[69, 500] loss: 0.012\n",
      "[69, 600] loss: 0.009\n",
      "[70, 100] loss: 0.018\n",
      "[70, 200] loss: 0.014\n",
      "[70, 300] loss: 0.015\n",
      "[70, 400] loss: 0.011\n",
      "[70, 500] loss: 0.009\n",
      "[70, 600] loss: 0.012\n",
      "[71, 100] loss: 0.016\n",
      "[71, 200] loss: 0.009\n",
      "[71, 300] loss: 0.016\n",
      "[71, 400] loss: 0.013\n",
      "[71, 500] loss: 0.009\n",
      "[71, 600] loss: 0.011\n",
      "[72, 100] loss: 0.018\n",
      "[72, 200] loss: 0.013\n",
      "[72, 300] loss: 0.008\n",
      "[72, 400] loss: 0.010\n",
      "[72, 500] loss: 0.013\n",
      "[72, 600] loss: 0.012\n",
      "[73, 100] loss: 0.021\n",
      "[73, 200] loss: 0.011\n",
      "[73, 300] loss: 0.011\n",
      "[73, 400] loss: 0.014\n",
      "[73, 500] loss: 0.013\n",
      "[73, 600] loss: 0.008\n",
      "[74, 100] loss: 0.015\n",
      "[74, 200] loss: 0.012\n",
      "[74, 300] loss: 0.013\n",
      "[74, 400] loss: 0.011\n",
      "[74, 500] loss: 0.009\n",
      "[74, 600] loss: 0.008\n",
      "[75, 100] loss: 0.021\n",
      "[75, 200] loss: 0.010\n",
      "[75, 300] loss: 0.009\n",
      "[75, 400] loss: 0.016\n",
      "[75, 500] loss: 0.007\n",
      "[75, 600] loss: 0.008\n",
      "[76, 100] loss: 0.020\n",
      "[76, 200] loss: 0.019\n",
      "[76, 300] loss: 0.008\n",
      "[76, 400] loss: 0.011\n",
      "[76, 500] loss: 0.012\n",
      "[76, 600] loss: 0.013\n",
      "[77, 100] loss: 0.017\n",
      "[77, 200] loss: 0.009\n",
      "[77, 300] loss: 0.021\n",
      "[77, 400] loss: 0.008\n",
      "[77, 500] loss: 0.019\n",
      "[77, 600] loss: 0.014\n",
      "[78, 100] loss: 0.012\n",
      "[78, 200] loss: 0.010\n",
      "[78, 300] loss: 0.013\n",
      "[78, 400] loss: 0.007\n",
      "[78, 500] loss: 0.008\n",
      "[78, 600] loss: 0.009\n",
      "[79, 100] loss: 0.014\n",
      "[79, 200] loss: 0.012\n",
      "[79, 300] loss: 0.009\n",
      "[79, 400] loss: 0.009\n",
      "[79, 500] loss: 0.006\n",
      "[79, 600] loss: 0.020\n",
      "[80, 100] loss: 0.016\n",
      "[80, 200] loss: 0.013\n",
      "[80, 300] loss: 0.013\n",
      "[80, 400] loss: 0.006\n",
      "[80, 500] loss: 0.012\n",
      "[80, 600] loss: 0.011\n",
      "[81, 100] loss: 0.019\n",
      "[81, 200] loss: 0.011\n",
      "[81, 300] loss: 0.011\n",
      "[81, 400] loss: 0.016\n",
      "[81, 500] loss: 0.015\n",
      "[81, 600] loss: 0.013\n",
      "[82, 100] loss: 0.019\n",
      "[82, 200] loss: 0.007\n",
      "[82, 300] loss: 0.018\n",
      "[82, 400] loss: 0.010\n",
      "[82, 500] loss: 0.009\n",
      "[82, 600] loss: 0.006\n",
      "[83, 100] loss: 0.015\n",
      "[83, 200] loss: 0.007\n",
      "[83, 300] loss: 0.005\n",
      "[83, 400] loss: 0.010\n",
      "[83, 500] loss: 0.009\n",
      "[83, 600] loss: 0.009\n",
      "[84, 100] loss: 0.017\n",
      "[84, 200] loss: 0.006\n",
      "[84, 300] loss: 0.008\n",
      "[84, 400] loss: 0.008\n",
      "[84, 500] loss: 0.009\n",
      "[84, 600] loss: 0.014\n",
      "[85, 100] loss: 0.018\n",
      "[85, 200] loss: 0.011\n",
      "[85, 300] loss: 0.023\n",
      "[85, 400] loss: 0.006\n",
      "[85, 500] loss: 0.010\n",
      "[85, 600] loss: 0.011\n",
      "[86, 100] loss: 0.024\n",
      "[86, 200] loss: 0.015\n",
      "[86, 300] loss: 0.011\n",
      "[86, 400] loss: 0.013\n",
      "[86, 500] loss: 0.008\n",
      "[86, 600] loss: 0.011\n",
      "[87, 100] loss: 0.019\n",
      "[87, 200] loss: 0.012\n",
      "[87, 300] loss: 0.006\n",
      "[87, 400] loss: 0.009\n",
      "[87, 500] loss: 0.010\n",
      "[87, 600] loss: 0.006\n",
      "[88, 100] loss: 0.018\n",
      "[88, 200] loss: 0.013\n",
      "[88, 300] loss: 0.016\n",
      "[88, 400] loss: 0.009\n",
      "[88, 500] loss: 0.008\n",
      "[88, 600] loss: 0.007\n",
      "[89, 100] loss: 0.012\n",
      "[89, 200] loss: 0.010\n",
      "[89, 300] loss: 0.012\n",
      "[89, 400] loss: 0.008\n",
      "[89, 500] loss: 0.012\n",
      "[89, 600] loss: 0.011\n",
      "[90, 100] loss: 0.014\n",
      "[90, 200] loss: 0.008\n",
      "[90, 300] loss: 0.011\n",
      "[90, 400] loss: 0.004\n",
      "[90, 500] loss: 0.006\n",
      "[90, 600] loss: 0.011\n",
      "[91, 100] loss: 0.014\n",
      "[91, 200] loss: 0.008\n",
      "[91, 300] loss: 0.010\n",
      "[91, 400] loss: 0.012\n",
      "[91, 500] loss: 0.019\n",
      "[91, 600] loss: 0.014\n",
      "[92, 100] loss: 0.019\n",
      "[92, 200] loss: 0.010\n",
      "[92, 300] loss: 0.009\n",
      "[92, 400] loss: 0.010\n",
      "[92, 500] loss: 0.010\n",
      "[92, 600] loss: 0.011\n",
      "[93, 100] loss: 0.014\n",
      "[93, 200] loss: 0.010\n",
      "[93, 300] loss: 0.008\n",
      "[93, 400] loss: 0.010\n",
      "[93, 500] loss: 0.010\n",
      "[93, 600] loss: 0.009\n",
      "[94, 100] loss: 0.014\n",
      "[94, 200] loss: 0.010\n",
      "[94, 300] loss: 0.011\n",
      "[94, 400] loss: 0.005\n",
      "[94, 500] loss: 0.016\n",
      "[94, 600] loss: 0.007\n",
      "[95, 100] loss: 0.017\n",
      "[95, 200] loss: 0.004\n",
      "[95, 300] loss: 0.013\n",
      "[95, 400] loss: 0.008\n",
      "[95, 500] loss: 0.009\n",
      "[95, 600] loss: 0.014\n",
      "[96, 100] loss: 0.013\n",
      "[96, 200] loss: 0.006\n",
      "[96, 300] loss: 0.005\n",
      "[96, 400] loss: 0.010\n",
      "[96, 500] loss: 0.015\n",
      "[96, 600] loss: 0.011\n",
      "[97, 100] loss: 0.010\n",
      "[97, 200] loss: 0.005\n",
      "[97, 300] loss: 0.012\n",
      "[97, 400] loss: 0.014\n",
      "[97, 500] loss: 0.005\n",
      "[97, 600] loss: 0.011\n",
      "[98, 100] loss: 0.014\n",
      "[98, 200] loss: 0.013\n",
      "[98, 300] loss: 0.006\n",
      "[98, 400] loss: 0.009\n",
      "[98, 500] loss: 0.010\n",
      "[98, 600] loss: 0.008\n",
      "[99, 100] loss: 0.014\n",
      "[99, 200] loss: 0.007\n",
      "[99, 300] loss: 0.007\n",
      "[99, 400] loss: 0.009\n",
      "[99, 500] loss: 0.009\n",
      "[99, 600] loss: 0.009\n",
      "[100, 100] loss: 0.013\n",
      "[100, 200] loss: 0.005\n",
      "[100, 300] loss: 0.005\n",
      "[100, 400] loss: 0.014\n",
      "[100, 500] loss: 0.007\n",
      "[100, 600] loss: 0.014\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 100\n",
    "running_loss = 0.0\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for i, data in enumerate(train_loader, 0):\n",
    "        inputs, labels = data\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = model(inputs.float())\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        if i % 100 == 99:\n",
    "            print(f\"[{epoch + 1}, {i + 1}] loss: {running_loss / 100:.3f}\")\n",
    "            running_loss = 0.0\n",
    "\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "933c968f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "predictions = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for data in test_loader:\n",
    "        data = data.to(device)\n",
    "\n",
    "        outputs = model(data)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        predictions.extend(predicted.cpu().tolist())\n",
    "\n",
    "submission = pd.DataFrame({\n",
    "    \"ImageId\": range(1, len(predictions) + 1),\n",
    "    \"Label\": predictions\n",
    "})\n",
    "\n",
    "submission.to_csv('submission.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
